# Redis Caching - How It Works

## Overview

Redis caching is **ENABLED** and **WORKING** in your application with tenant isolation and automatic fallback.

## Architecture

```
API Request
    ‚Üì
Dashboard Endpoint (src/api/dashboard.py)
    ‚Üì
Check Redis Cache (src/services/cache_service.py)
    ‚Üì
‚îú‚îÄ Cache HIT ‚Üí Return cached data (< 10ms)
‚îî‚îÄ Cache MISS ‚Üí Fetch from Mock API ‚Üí Store in Redis ‚Üí Return data
```

## Configuration

### 1. Environment Variables (`.env`)
```bash
REDIS_URL=redis://172.27.52.219:6379/0  # WSL IP address
CACHE_ENABLED=True
```

### 2. Application Settings (`src/config.py`)
```python
redis_url: str = "redis://localhost:6379/0"
cache_enabled: bool = True
```

### 3. Redis Connection (WSL Ubuntu)
- **Location**: Running in WSL Ubuntu
- **IP**: 172.27.52.219 (WSL network interface)
- **Port**: 6379
- **Database**: 0
- **Binding**: 0.0.0.0 (all interfaces)
- **Protected Mode**: Disabled (for development)

## How Redis is Initialized

### Startup Sequence (`src/main.py`)

```python
@app.on_event("startup")
async def startup_event():
    # 1. Check if cache is enabled
    if settings.cache_enabled:
        # 2. Create CacheManager instance
        cache_manager = CacheManager(redis_url=settings.redis_url)
        
        # 3. Connect to Redis
        await cache_manager.connect()
        
        # 4. Set global cache manager
        if cache_manager._redis:
            logger.info("Redis cache initialized")
            set_cache_manager(cache_manager)
        else:
            logger.warning("Redis unavailable. Continuing without cache.")
            set_cache_manager(None)
```

### Connection Details (`src/cache/cache_manager.py`)

```python
async def connect(self):
    # Create Redis client
    self._redis = await redis.from_url(
        self.redis_url,
        encoding="utf-8",
        decode_responses=True
    )
    
    # Test connection
    await self._redis.ping()
    
    # Configure LRU eviction
    await self._configure_lru_eviction()
```

### LRU Configuration

```python
async def _configure_lru_eviction(self):
    # Set max memory (256MB default)
    max_memory_bytes = 256 * 1024 * 1024
    await self._redis.config_set('maxmemory', max_memory_bytes)
    
    # Set eviction policy to LRU (Least Recently Used)
    await self._redis.config_set('maxmemory-policy', 'allkeys-lru')
```

## Cache Service API

### Simple API (`src/services/cache_service.py`)

```python
# Build cache key with tenant isolation
cache_key = make_cache_key("dashboard_kpis", tenant_id)
# Result: "dashboard_kpis:tenant-001:..."

# Check cache
cached_data = await get_cached(cache_key)
if cached_data is not None:
    return cached_data  # Cache HIT

# Cache miss - fetch from source
data = await fetch_from_mock_api()

# Store in cache with TTL
await set_cached(cache_key, data, ttl=300)  # 5 minutes

return data
```

### Cache Key Format

```
{endpoint}:{tenant_id}:{optional_params}

Examples:
- dashboard_kpis:54d459ab-4ae8-480a-9d1c-d53b218a4fb2
- dashboard_trends:54d459ab-4ae8-480a-9d1c-d53b218a4fb2:days:30
- dashboard_activity:54d459ab-4ae8-480a-9d1c-d53b218a4fb2:limit:10
```

## TTL (Time-To-Live) Configuration

```python
CACHE_TTL = {
    "dashboard_kpis": 300,      # 5 minutes
    "dashboard_trends": 300,    # 5 minutes
    "dashboard_alerts": 300,    # 5 minutes
    "dashboard_insights": 300,  # 5 minutes
    "dashboard_stats": 300,     # 5 minutes
    "dashboard_activity": 300,  # 5 minutes
    "products": 600,            # 10 minutes
    "pricing_trends": 600,      # 10 minutes
    "sentiment_overview": 900,  # 15 minutes
    "forecast_demand": 1800,    # 30 minutes
}
```

## Dashboard Endpoint Example

### Code (`src/api/dashboard.py`)

```python
@router.get("/kpis")
async def get_dashboard_kpis(request: Request, db: AsyncSession = Depends(get_db)):
    tenant_id = str(get_tenant_id_from_request(request))
    email = get_user_email_from_request(request)
    
    # Step 1: Build cache key
    cache_key = make_cache_key("dashboard_kpis", tenant_id)
    
    # Step 2: Check cache
    cached_data = await get_cached(cache_key)
    if cached_data is not None:
        logger.info(f"‚úÖ Cache HIT: {cache_key}")
        return {"payload": cached_data}
    
    # Step 3: Cache miss - fetch from DataService
    logger.info(f"‚ùå Cache MISS: {cache_key}")
    data_service = DataService(tenant_id=tenant_id, email=email, db=db)
    kpis = await data_service.get_dashboard_kpis()
    
    # Step 4: Store in cache
    await set_cached(cache_key, kpis, ttl=get_ttl("dashboard_kpis"))
    logger.info(f"üíæ Cache SET: {cache_key} (TTL: 300s)")
    
    # Step 5: Return
    return {"payload": kpis}
```

### Request Flow

**First Request (Cache MISS):**
```
1. Request: GET /api/v1/dashboard/kpis
2. Check Redis: dashboard_kpis:tenant-001 ‚Üí NOT FOUND
3. Fetch from Mock API: http://localhost:8001/api/v1/dashboard/stats
4. Store in Redis: dashboard_kpis:tenant-001 ‚Üí {data} (TTL: 300s)
5. Response time: ~500ms
```

**Second Request (Cache HIT):**
```
1. Request: GET /api/v1/dashboard/kpis
2. Check Redis: dashboard_kpis:tenant-001 ‚Üí FOUND
3. Return cached data
4. Response time: <10ms
```

## Tenant Isolation

Every cache key includes the tenant_id to ensure data isolation:

```python
# Tenant A
cache_key = "dashboard_kpis:tenant-001"

# Tenant B
cache_key = "dashboard_kpis:tenant-002"
```

This ensures:
- Tenant A can NEVER see Tenant B's cached data
- Each tenant has their own cache namespace
- Cache invalidation is tenant-specific

## Cache Invalidation

### Manual Invalidation

```python
# Invalidate specific key
await invalidate_cache("dashboard_kpis:tenant-001")

# Invalidate all keys for a tenant
await invalidate_tenant_cache("tenant-001")

# Invalidate by pattern
await invalidate_tenant_cache("tenant-001", prefix="dashboard")
```

### Automatic Invalidation (Event-Driven)

When data changes (e.g., product updated, order created), the cache is automatically invalidated:

```python
# Event published
event_bus.publish("product.updated", product_id="123")

# Cache subscriber invalidates related caches
await cache_manager.invalidate_pattern(f"products:{tenant_id}:*")
```

## Graceful Fallback

If Redis is unavailable, the application continues to work:

```python
async def get_cached(key: str) -> Optional[Any]:
    cache_manager = get_cache_manager()
    
    # Redis not available
    if not cache_manager or not cache_manager._redis:
        logger.warning(f"Redis not connected for key: {key}")
        return None  # Cache MISS - fetch from source
    
    # Continue with Redis...
```

**Behavior:**
- Every request becomes a cache MISS
- Data is fetched from Mock API every time
- No errors shown to user
- Slightly slower response times (~500ms vs <10ms)

## Testing Redis Connection

### Test Script (`scripts/test_redis.py`)

```bash
python scripts/test_redis.py
```

**Expected Output:**
```
Testing Redis connection...
Redis URL: redis://172.27.52.219:6379/0
--------------------------------------------------
Test 1: Ping Redis...
‚úÖ Redis ping successful! Result: True

Test 2: Set a test key...
‚úÖ Redis set successful!

Test 3: Get the test key...
‚úÖ Redis get successful! Value: hello_from_windows

Test 4: Delete the test key...
‚úÖ Redis delete successful!

==================================================
‚úÖ ALL TESTS PASSED - Redis is fully working!
==================================================
```

### Manual Test (WSL)

```bash
# In WSL Ubuntu terminal
redis-cli ping
# Expected: PONG

redis-cli
> SET test_key "hello"
> GET test_key
> DEL test_key
> EXIT
```

## Monitoring Cache Performance

### Backend Logs

```
# Cache MISS (first request)
‚ùå Cache MISS: dashboard_kpis:tenant-001
Fetching from Mock API...
üíæ Cache SET: dashboard_kpis:tenant-001 (TTL: 300s)

# Cache HIT (subsequent requests)
‚úÖ Cache HIT: dashboard_kpis:tenant-001
Response time: <10ms
```

### Cache Metrics Endpoint

```bash
GET /api/v1/cache/metrics
```

**Response:**
```json
{
  "hits": 150,
  "misses": 50,
  "total_requests": 200,
  "hit_rate_percent": 75.0,
  "used_memory_mb": 12.5,
  "max_memory_mb": 256,
  "memory_usage_percent": 4.88,
  "redis_evicted_keys": 0
}
```

## Benefits

### Performance
- **Cache HIT**: <10ms response time
- **Cache MISS**: ~500ms (Mock API fetch)
- **Improvement**: 50x faster for cached data

### Scalability
- Reduces load on Mock API
- Handles more concurrent users
- Lower latency for end users

### Cost Savings
- Fewer API calls to external services
- Reduced compute resources
- Lower bandwidth usage

## Summary

‚úÖ **Redis is ENABLED and WORKING**
- Connected to WSL Redis at 172.27.52.219:6379
- LRU eviction configured (256MB max memory)
- Tenant isolation with cache keys
- Graceful fallback if Redis unavailable
- All dashboard endpoints use caching
- TTL: 5-30 minutes depending on endpoint

‚úÖ **Cache Flow**
1. Check Redis cache
2. If HIT ‚Üí Return instantly (<10ms)
3. If MISS ‚Üí Fetch from Mock API ‚Üí Store in Redis ‚Üí Return
4. Automatic invalidation on data changes

‚úÖ **Tenant Isolation**
- Every cache key includes tenant_id
- Sellers can only see their own cached data
- Cache invalidation is tenant-specific
